{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Gradient Descent method (most important lesson in machine learning)\n",
    "\n",
    "* Basic probelm in machine learning is to find best model for a certain situation\n",
    "* BEST will mean something like **minimizes the error between model and data**. \n",
    "$$\\sum\\limits_{i=1}^n e_{i}^2 = \\sum\\limits_{i=1}^n (y_{i}^{actual} - y_{i}^{model})^2 = \\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2$$\n",
    "* In other words, it will represent the solution to some sort of optimization problem.\n",
    "* Finding the optimum solution requires a technique called gradient descent\n",
    "* We will explore the use of gradient descent to determine our parameters for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"img/least-squares2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"img/best_fitline.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "<ul>\n",
    "    <li> <a href=\"#gripsearch\"> Grid Search </a> </li>\n",
    "    <li> <a href=\"#gradientdescent\"> Gradient Descent  </a> </li>\n",
    "    <li> <a href=\"#multivariate\"> Multivariate linear regression  </a> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats, polyfit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gridsearch'></a>\n",
    "# Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One dimensional example\n",
    "\n",
    "let us say we have following function \n",
    "\n",
    "$$f(x) = 2x^{2} + 2x$$\n",
    "\n",
    "Now, I want to find the value of x where this function is minimum\n",
    "\n",
    "$$\\frac{\\partial }{\\partial x} f(x) = 4x + 2 = 0$$\n",
    "\n",
    "which gives $$x = -0.5$$\n",
    "\n",
    "So analytical solution gives $x=-0.5$, now we will try to find this value numerically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_onedim(x):\n",
    "    return 2*x*x + 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,4000)\n",
    "y = func_onedim(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,linestyle='', marker='o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('variation of f(x) as a function of x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_index = np.argmin(y)\n",
    "print(\"Minimum value of f(x) lies at x = {:.3f}\".format(x[min_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us find the minimum value of function using standard scipy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "result = minimize(func_onedim, x0=-1)\n",
    "print(\"Minimum value of f(x) lies at x = {:.3f}\".format(result.x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to create a linear regression that follows the following relation:\n",
    "\n",
    "$$y=\\beta_{0}+\\beta_{1}x$$\n",
    "where:\n",
    "$$\\beta_{0}=5$$\n",
    "$$\\beta_{1}=2$$\n",
    "\n",
    "Let’s start by creating some mock data and we’ll graph up this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets us apply the grid search method for our linear regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = 5\n",
    "beta_1 = 2\n",
    "x= np.random.randint(10,100, 200)\n",
    "y = beta_0 + beta_1*x\n",
    "\n",
    "# Add some random error\n",
    "\n",
    "y = y + np.random.normal(0,10, len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, linestyle='', marker='o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('x vs y regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard notation is:\n",
    "\n",
    "$$y^{model} = \\beta_{0} + \\beta_{1}x$$\n",
    "\n",
    "where $\\beta_{0}$, $\\beta_{1}$ are regression coefficients we need to determine\n",
    "\n",
    "Lets us define the error as;\n",
    "\n",
    "$$e_{i} = y_{i}^{actual} - y_{i}^{model}$$ \n",
    "\n",
    "From this error definition, we can find the expression for square of error terms for all data points as:\n",
    "\n",
    "$$cost function \\ (Mean \\ Squared \\ error) = \\frac{1}{2n}\\sum\\limits_{i=1}^n e_{i}^2 = \\frac{1}{2n} \\sum\\limits_{i=1}^n (y_{i}^{actual} - y_{i}^{model})^2 = \\frac{1}{2n} \\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assigment related to cost function</font>\n",
    "1. Could you name any other cost function which you might use in regression problem?\n",
    "2. Why Mean Squared error function is a good cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try each integer for beta 0 and beta 1 some numbers\n",
    "beta_0 = np.linspace(beta_0-3, beta_0+3, 50)\n",
    "beta_1 = np.linspace(beta_1-5, beta_1+5, 50)\n",
    "\n",
    "\n",
    "# All combinations of beta_0 and beta_1\n",
    "plt_beta_0, plt_beta_1 = np.meshgrid(beta_0, beta_1)  # use need to understand this, plz play with this function\n",
    "\n",
    "def calculate_mse(beta_0, beta_1):\n",
    "    y_hat = beta_0 + beta_1 * x\n",
    "    error = y_hat - y\n",
    "    sse = np.sum(error ** 2)\n",
    "    return ((1 / (2 * len(x))) * sse)\n",
    "\n",
    "calculate_mse_v = np.vectorize(calculate_mse)\n",
    "mse = calculate_mse_v(plt_beta_0, plt_beta_1)#.reshape(len(plt_beta_0), len(plt_beta_1))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(plt_beta_0,\n",
    "                       plt_beta_1,\n",
    "                       mse,\n",
    "                       cmap='viridis',\n",
    "                       linewidth=0,\n",
    "                       antialiased=False)\n",
    "ax.set_xlabel(r'$\\hat{\\beta_0}$')\n",
    "ax.set_ylabel(r'$\\hat{\\beta_1}$')\n",
    "ax.set_zlabel(r'MSE')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the indexes of $\\beta_{0}$ and $\\beta_{1}$ where the cost function is minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index for the values of beta_0 and beta_1\n",
    "# For which SSE is lowest\n",
    "beta_1_idx, beta_0_idx = np.unravel_index(mse.argmin(), mse.shape)\n",
    "\n",
    "\n",
    "# Retrieve values of beta_0 and beta_1 for which\n",
    "# SSE is lowest\n",
    "beta_0_hat = beta_0[beta_0_idx]\n",
    "beta_1_hat = beta_1[beta_1_idx]\n",
    "\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Grid Search solution y = {} + {}x\".format(beta_0_hat, beta_1_hat))\n",
    "\n",
    "\n",
    "# Plot a line for our model\n",
    "plt.scatter(x, y)\n",
    "plt.plot(\n",
    "    [min(x), max(x)],\n",
    "    [min(x) * beta_1_hat + beta_0_hat, max(x) * beta_1_hat + beta_0_hat],\n",
    "    color='blue'\n",
    ")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical solution to find minimum using scipy.optimize.minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(beta, x ,y):\n",
    "    model = beta[0] + beta[1]*x\n",
    "    error = y - model\n",
    "    return np.sum(error**2) / (2*len(x))\n",
    "    \n",
    "results = minimize(linear_regression, [0,4], args = (x,y), method='L-BFGS-B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical solution using scipy,  y = {:.2f} + {:.2f}x\".format(results.x[0], results.x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker='s', color='green')\n",
    "plt.plot(\n",
    "    [min(x), max(x)],\n",
    "    [min(x) * beta_1_hat + beta_0_hat, max(x) * beta_1_hat + beta_0_hat],\n",
    "    color='blue', label='Grid Search method'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [min(x), max(x)],\n",
    "    [min(x) * results.x[1] + results.x[0], max(x) * results.x[1] + results.x[0]],\n",
    "    color='red', label='Numerical Solution'\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have notice that although it is easy to understand the grid search method to find the minimum in a cost fuction, but this is very inefficient, and quickly becomes slower as you increase granularity and search space. To overcome problem of inefficiney, we will try gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gradientdescent'></a>\n",
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative process:\n",
    "\n",
    "1. Initialise $\\beta_{0}$ and $\\beta_{1}$ with random values and calculate MSE\n",
    "2. Calculate the gradient so we move in the direction of minimising MSE\n",
    "3. Adjust the $\\beta_{0}$ and $\\beta_{1}$ with gradient\n",
    "4. Use new weights to get values for $y_{model}$  to calculate MSE\n",
    "5. Repeat steps 2-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\beta_{i}= \\beta_{i} - \\alpha\\frac{\\partial }{\\partial \\beta_{i}} MSE(\\beta_{0},\\beta_{1})$$\n",
    "where \"i\" have two value 0 and 1. \"$\\alpha$\" defines the learning rate (How fast you want to move down the slope)\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_{0}} MSE(\\beta_{0},\\beta_{1}) = \\frac{\\partial }{\\partial \\beta_{0}} \\frac{1}{2n}\\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2 = - \\frac{1}{n}\\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_{1}} MSE(\\beta_{0},\\beta_{1}) = \\frac{\\partial }{\\partial \\beta_{1}} \\frac{1}{2n}\\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2 = - \\frac{1}{n}\\sum\\limits_{i=1}^n x_{i} (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gradient_cost_function.png\" width=600, align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.ones(len(x)), x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.00001\n",
    "iterations = 150\n",
    "beta_0_hat = 5\n",
    "beta_1_hat = 1\n",
    "mses = []\n",
    "y_hat_list = []\n",
    "\n",
    "for i in range(1, iterations+1):\n",
    "    y_hat = beta_0_hat * X[0] + beta_1_hat * X[1]\n",
    "    y_hat_list.append(y_hat)\n",
    "    error = y_hat - y\n",
    "    sse = np.sum(error ** 2)\n",
    "    mse = ((1 / (2 * len(X.T))) * sse)\n",
    "    mses.append(mse)\n",
    "    \n",
    "    gradient = np.dot(X, error) / len(X.T)\n",
    "    beta_0_hat = beta_0_hat - (gradient[0] * alpha)\n",
    "    beta_1_hat = beta_1_hat - (gradient[1] * alpha)\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(\"Iteration {}, MSE={}, β0={}, β1={}\".format(\n",
    "            i, round(mse, 3), round(beta_0_hat, 3), round(beta_1_hat, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[1], y)\n",
    "plt.plot(\n",
    "    [min(X[1]), max(X[1])],\n",
    "    [min(X[1]) * beta_1_hat + beta_0_hat, max(X[1]) * beta_1_hat + beta_0_hat],\n",
    "    color='blue'\n",
    ")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Cost Function J')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(mses[:8000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_xlim(( min(x)-5, max(x)+5))\n",
    "ax.set_ylim((int(min(y))-10, int(max(y))+10))\n",
    "points = ax.scatter(x, y, color='red')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "line, = ax.plot([], [], lw=2)\n",
    "epoch_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    line.set_data([], [])\n",
    "    epoch_text.set_text('epoch = 0')\n",
    "    return line, epoch_text\n",
    "  \n",
    "def animate(i):\n",
    "    line.set_data(x, y_hat_list[i])\n",
    "    epoch_text.set_text('epoch = {}'.format(i))\n",
    "    return line, epoch_text\n",
    "  \n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, repeat=False,\n",
    "                               frames=len(y_hat_list), interval=100, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge ffmpeg use this command if you are getting error to install ffmepg player\n",
    "from IPython.display import HTML\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/opt/local/anaconda/anaconda/envs/course-ml/bin/ffmpeg' # For google colab\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multivariate'></a>\n",
    "# Multivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to create a linear regression that follows the following relation:\n",
    "\n",
    "$$y=\\beta_{0}+\\beta_{1}x1 + \\beta_{2}x2$$\n",
    "where:\n",
    "$$\\beta_{0}=-10$$\n",
    "$$\\beta_{1}=2$$\n",
    "$$\\beta_{2}=3$$\n",
    "\n",
    "\n",
    "Let’s start by creating some mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = -10\n",
    "beta_1 = 4\n",
    "beta_2 = 2\n",
    "x1= np.random.randint(10,100, 200)\n",
    "\n",
    "x2= np.random.randint(-20,50, 200)\n",
    "\n",
    "y = beta_0 + beta_1*x1 + beta_2*x2 \n",
    "\n",
    "# Add some random error\n",
    "\n",
    "y = y + np.random.normal(0,10, len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.ones(len(x1)), x1,x2])\n",
    "df = pd.DataFrame(X.T)\n",
    "df.columns = [\"x0\", \"x1\", \"x2\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.00001\n",
    "iterations = 200\n",
    "beta_0_hat = -4\n",
    "beta_1_hat = 1\n",
    "beta_2_hat = 3\n",
    "mses = []\n",
    "y_hat_list = []\n",
    "\n",
    "for i in range(1, iterations+1):\n",
    "    y_hat = beta_0_hat * X[0] + beta_1_hat * X[1] + beta_2_hat * X[2]\n",
    "    y_hat_list.append(y_hat)\n",
    "    error = y_hat - y\n",
    "    sse = np.sum(error ** 2)\n",
    "    mse = ((1 / (2 * len(X.T))) * sse)\n",
    "    mses.append(mse)\n",
    "    \n",
    "    gradient = np.dot(X, error) / len(X.T)\n",
    "    beta_0_hat = beta_0_hat - (gradient[0] * alpha)\n",
    "    beta_1_hat = beta_1_hat - (gradient[1] * alpha)\n",
    "    beta_2_hat = beta_2_hat - (gradient[2] * alpha)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"Iteration {}, MSE={}, β0={}, β1={}, β2={} \".format(\n",
    "            i, round(mse, 3), round(beta_0_hat, 3), round(beta_1_hat, 3), round(beta_2_hat, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Cost Function J')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(mses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
