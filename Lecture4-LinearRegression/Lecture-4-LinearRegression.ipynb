{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "* Regression analysis is likely the first and simplest of all of predictive models we would use in this course\n",
    "* It estimates the relationship between a dependent variable we can also call it as target & an independent variable which is also known as predictor\n",
    "* First published in early 1800s by Legrendre and Gauss\n",
    "* Regression was coined by Francis Galton (cousin of Charles Darwin) to describe the biological process of extreme values moving towards population mean\n",
    "* It can be classified as a supervised learning algorithms\n",
    "* Very easy to interpret\n",
    "* It is also called least square method  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "<ul>\n",
    "    <li> <a href=\"#fakedata\"> Creating Data </a> </li>\n",
    "    <li> <a href=\"#fitting\"> Model Fitting </a> </li>\n",
    "    <li> <a href=\"#evaluation\"> Model Evaluation (R-Square value) </a> </li>\n",
    "    <li> <a href=\"#assumtions\"> Assumptions of linear regression</a> </li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import all the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats, polyfit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fakedata'></a>\n",
    "# Creating Data\n",
    "* we will create a some fake height vs weight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = np.random.uniform(40,75, 20)\n",
    "beta0 = 10\n",
    "beta1 = 2\n",
    "weight = beta0 + beta1*height\n",
    "\n",
    "# Add some random error\n",
    "\n",
    "weight  = weight + np.random.normal(0,10, len(height))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe for easy view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Height':height, 'Weight':weight})\n",
    "df = df.round(1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(figsize=(5,5))\n",
    "plt.plot(height,weight, linestyle='', marker='o')\n",
    "plt.xlabel('Height [inches]')\n",
    "plt.ylabel('Weight [Pounds]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fitting'></a>\n",
    "# Model Fitting\n",
    "* Method of least square fitting\n",
    "* Method determines the best line which minimizes the total of the square of the errors between the line and data points as small as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/least-squares2.svg\" width=300, align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics for least square fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard notation is:\n",
    "\n",
    "$$y^{model} = \\beta_{0} + \\beta_{1}x$$\n",
    "\n",
    "where $\\beta_{0}$, $\\beta_{1}$ are regression coefficients we need to determine\n",
    "\n",
    "Lets us define the error as;\n",
    "\n",
    "$$e_{i} = y_{i}^{actual} - y_{i}^{model}$$ \n",
    "\n",
    "From this error definition, we can find the expression for square of error terms for all data points as:\n",
    "\n",
    "$$\\sum\\limits_{i=1}^n e_{i}^2 = \\sum\\limits_{i=1}^n (y_{i}^{actual} - y_{i}^{model})^2 = \\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2$$\n",
    "\n",
    "We minimize the above expression with respect to $\\beta_{0}$ and $\\beta_{1}$, setting the derivative to zero. From there we can calculate the values of our regression coefficients.\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_{0}} \\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2 = 0$$\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_{1}} \\sum\\limits_{i=1}^n (y_{i}^{actual} - (\\beta_{0} + \\beta_{1}x_{i}))^2 = 0$$\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assigment related to Least Square method</font>\n",
    "1. Find the value of $\\beta_{0}$ and $\\beta_{1}$ from the above expression. That will be the analytical solution to the regression coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='black'>Lets us build our model using numpy polyfit routine which calculate the best fit line based on least square fitting method</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, cov_matrix = np.polyfit(df['Height'], df['Weight'], 1, cov=True)\n",
    "intercept, intercept_error = parameters[1], np.sqrt(cov_matrix[1][1])\n",
    "slope, slope_error = parameters[0], np.sqrt(cov_matrix[0][0])\n",
    "\n",
    "print(\"beta0 = {:.2g} +/- {:.2g} , beta1 = {:.2f} +/- {:.2g} \".format(intercept, intercept_error,slope, slope_error))\n",
    "\n",
    "print(\"Actual value\")\n",
    "\n",
    "print(\"beta0 = {:.2g}, beta1 = {:.2f}\".format(beta0, beta1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = np.min(df['Height']), np.max(df['Height'])\n",
    "x = np.linspace(x_min-5,x_max+5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(figsize=(8,8))\n",
    "plt.plot(df['Height'], df['Weight'], marker='o', linestyle='none', label='Data')\n",
    "plt.plot(x, x*slope + intercept, label=\"slope={:.2g}, intercept={:.2f}\".format(slope, intercept))\n",
    "plt.legend()\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Weight(Y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assigment related to model fitting</font>\n",
    "1. Compare the value of $\\beta_{0}$ and $\\beta_{1}$ from your calculation to the value calculated by the standard libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "# Model Evaluation (or Model Goodness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared value\n",
    "* R-Square value: the proportion of the variance in the dependent variable that is predictable from the independent variable.In other words, It is a statistical measure of how close the data are to the fitted regression line.\n",
    "* The best regression line we can get when R-Square is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$R^{2} = \\frac{\\sum\\limits_{i=1}^n (y^{model} - y^{mean})^2}{\\sum\\limits_{i=1}^n (y^{actual} - y^{mean})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(x,y,parameters):\n",
    "    ybar = np.mean(y)\n",
    "    ymodel = parameters[0]*x + parameters[1]\n",
    "    ssreg = np.sum((ymodel-ybar)**2)\n",
    "    sstot = np.sum((y - ybar)**2)\n",
    "    \n",
    "    return (ssreg / sstot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Sqaure value from our calculation = {:.2f}\".format(r2_score(height, weight, parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us compare this value with the standard package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(height, weight)\n",
    "print(\"R-squared from standard package: {:.3f}\".format(r_value**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot\n",
    "* The residual plot is the most important plot to review the acceptance of prediction from linear regression model\n",
    "* A proper residual plot will look like a random scatter of points around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=(12,6))\n",
    "sns.regplot(x='Height', y='Weight', data=df, ax = axes[0])\n",
    "sns.residplot(x='Height', y='Weight', data=df, ax = axes[1])\n",
    "\n",
    "axes[0].set_title('Regression plot')\n",
    "axes[1].set_title('Residual plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assigment related to model evaluation</font>\n",
    "1. Write a function for reduced R-Squared value. \n",
    "2. Explain why Reduced R-Squared (or Adjusted R-Squared) is a better measure for model goodness than R-Squared\n",
    "3. Write a plot function to plot the residual plot and compare it with above plot. Do you see the same plot from your own function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "# Assumptions of linear regression\n",
    "* There should not be a measurement error in the observed values of the target and each input variable.\n",
    "* The relationship between the dependent variable and all of indepenent variables are linear\n",
    "* The observations should be independent\n",
    "* The variance of the target variable should be constant (Homoscedasticity)\n",
    "* Outlier should be treated properly, otherwise it will biased your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violation of Homoscedasticity\n",
    "The residual plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The following scatter plots show examples of data that are not homoscedastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/homoscedasticity.png\" width=500, align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = np.random.uniform(40,75, 100)\n",
    "beta0 = 10\n",
    "beta1 = 2\n",
    "weight = beta0 + beta1*height\n",
    "\n",
    "# Add some random error\n",
    "new_weight = np.zeros(len(height))\n",
    "for i in range(len(height)):\n",
    "    variance = int(weight[i] / 20)\n",
    "    \n",
    "    new_weight[i]  = weight[i] + np.random.normal(0,variance**4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Height':height, 'Weight':new_weight})\n",
    "df = df.round(1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=(12,6))\n",
    "sns.regplot(x='Height', y='Weight', data=df, ax = axes[0])\n",
    "sns.residplot(x='Height', y='Weight', data=df, ax = axes[1])\n",
    "\n",
    "axes[0].set_title('Regression plot')\n",
    "axes[1].set_title('Residual plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violation of linear relation\n",
    "\n",
    "* if curvature is present in the residuals, then it is likely that there is curvature in the relationship between the response and the predictor that is not explained by our model.  \n",
    "* A linear model does not adequately describe the relationship between the predictor and the response\n",
    "\n",
    "<img src=\"img/linearity.png\" width=500, align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = np.random.uniform(40,75, 50)\n",
    "beta0 = 10\n",
    "beta1 = 2\n",
    "weight = beta0 + beta1*height*height + np.random.normal(0,10, len(height))\n",
    "\n",
    "df = pd.DataFrame({'Height':height, 'Weight':weight})\n",
    "df = df.round(1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=(12,6))\n",
    "sns.regplot(x='Height', y='Weight', data=df, ax = axes[0])\n",
    "sns.residplot(x='Height', y='Weight', data=df, ax = axes[1])\n",
    "\n",
    "axes[0].set_title('Regression plot')\n",
    "axes[1].set_title('Residual plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Outlier\n",
    "\n",
    "* Outliers can have a big influence on the fit of the regression line\n",
    "* The one extreme outlier is essentially tilting the regression line\n",
    "* As a result, the model will not predict well for many of the observations.\n",
    "\n",
    "<img src=\"img/outlier.png\" width=500, align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,5,10)\n",
    "y = 2*x + 5 + np.random.normal(0,3,size=len(x))\n",
    "\n",
    "x_outlier = np.append(x,4)\n",
    "y_outlier = np.append(y,200)\n",
    "\n",
    "slope, intercept = np.polyfit(x,y,1)\n",
    "slope_outlier, intercept_outlier = np.polyfit(x_outlier, y_outlier,1)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6,6))\n",
    "\n",
    "plt.plot(x_outlier, y_outlier, marker='o', linestyle='none', label='Data')\n",
    "\n",
    "plt.plot(x, x*slope_outlier+intercept_outlier, label='Outlier')\n",
    "\n",
    "plt.plot(x, x*slope+intercept, label='Removing Outlier')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('X [arbitrary units]')\n",
    "plt.ylabel('Y [arbitrary units]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
