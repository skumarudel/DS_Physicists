{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets create some fake data first to do some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use Python to generate some toy data to demonstrate the two approaches to the problem. Because the measurements are number counts, a Poisson distribution is a good approximation to the measurement process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_true = 100 #true flux, say number of photons detected per sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200 # number of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = stats.poisson(F_true).rvs(N)  # N measurements of the flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.sqrt(F)  # errors on Poisson counts estimated via square root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a simple visualization of the \"measured\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(F, np.arange(N), xerr=e, fmt='ok', ecolor='gray', alpha=0.5)\n",
    "ax.vlines([F_true], 0, N, linewidth=5, alpha=0.5)\n",
    "ax.set_xlabel(\"Flux\");ax.set_ylabel(\"measurement number\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel(\"Flux\");ax.set_ylabel(\"measurement number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we know the true flux in this case \"FORGET ABOUT THAT FOR NOW\"\n",
    "The real question is this: given our measurements and errors, what is our best estimate of the true flux?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist approach(unbinned liklihood analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the classical maximum liklihood approach to find the parameter of interest(Photon flux).Given a single observation $Di=(Fi,ei)$, we can compute the probability of this measurement given the true flux $F_{true}$ and Gaussian errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(D_{i}| F_{true}) = \\frac{1}{\\sqrt{2\\pi e_{i}^2}}\\exp\\Big[\\frac{-(F_{i} - F_{true})^2}{2e_{i}^2}\\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the likelihood function by computing the product of the probabilities for each data point:\n",
    "$$ L(D | F_{true}) = \\prod\\limits_{i=1}^n P(D_{i} | F_{true})$$\n",
    "\n",
    "Here D represents the entire set of measurements.Because the value of the likelihood can become very small, it is often more convenient to instead compute the log-likelihood. Combining the previous two equations and computing the log\n",
    "\n",
    "$$\\log L = -\\frac{1}{2} \\sum\\limits_{i=1}^n \\Big[ \\log(2\\pi e_{i}^2) + \\frac {(F_{i} - F_{true})^2}{e_{i}^2}\\Big]$$\n",
    "\n",
    "What we'd like to do is determine $F_{true}$ such that the likelihood is maximized. For this simple problem, the maximization can be done numerically or analytically. I will use python to find the maximum if the function. Let us first see the graph to get an idea what I am doing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_ParSpace = np.linspace(F_true - 200,F_true + 200,401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikihood(Ftrue):\n",
    "    sum = 0\n",
    "    for i in range(0, len(F)):\n",
    "        sum += np.log(2*np.pi*e[i]) + ((F[i] - Ftrue)**2)/(e[i]**2)\n",
    "    \n",
    "    return 0.5*sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x = -1*loglikihood(F_ParSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F_ParSpace,p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " opt_fn = lambda S: loglikihood(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(opt_fn,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmax = result.x[0]  # This gives the value of parameter which gives minimum of logliklihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.fun          # This gives the value of minimum value of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = lambda x: loglikihood(x) - loglikihood(logmax) - 0.5  # defining function to find the error on the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_error = scipy.optimize.brentq(function, logmax, F_true+ 200)  # finding the roots where the liklihood decrease by 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F_estimated = \" + str(logmax) + \" +/- \" + str(S_error - logmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the 1 Sigma confidence interval says that we are 68 percent confident that the true mean lies in this interval.It does not talk about the probability. if we consider a 95% confidence interval,Then 95% of all the realized 95% confidence intervals calculated in independent measurement problems will contain the actual values of the measurands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what is a Bayes' Theorem, a fundamental law of probability:\n",
    "\n",
    "$$P(F_{True} | D) = \\frac{P(D | F_{True}) P(F_{True})}{P(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the prior P(Ftrue)$\\propto$1 (a flat prior), we find\n",
    "\n",
    "$$P(Ftrue|D) \\propto L(D|Ftrue)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the Bayesian probability is maximized at precisely the same value as the frequentist result! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(Ftrue):\n",
    "    mmu = 80\n",
    "    msigma = 1\n",
    "    if Ftrue < 0:  #out the permissible range Prob is zero and log(Prob) become inf\n",
    "        return -np.inf\n",
    "    return -0.5*((Ftrue - mmu)/msigma)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def log_prior(Ftrue):\n",
    " #   return 1  # flat prior\n",
    "\n",
    "def log_likelihood(Ftrue, F, e):\n",
    "    return -0.5 * np.sum(np.log(2 * np.pi * e ** 2)\n",
    "                         + (F - Ftrue) ** 2 / e ** 2)\n",
    "\n",
    "def log_posterior(Ftrue, F, e):\n",
    "    return log_prior(Ftrue) + log_likelihood(Ftrue, F, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_ParSpace = np.linspace(F_true - 50,F_true + 50,401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(figsize=(8,6))\n",
    "prior = [log_prior(S) for S in F_ParSpace]\n",
    "likelihood = [log_likelihood(S,F,e)for S in F_ParSpace]\n",
    "posterior = [log_likelihood(S,F,e) + log_prior(S) for S in F_ParSpace]\n",
    "\n",
    "\n",
    "axes.plot(F_ParSpace, prior, label='Prior')\n",
    "axes.plot(F_ParSpace, likelihood, label='Likelihood')\n",
    "axes.plot(F_ParSpace, posterior, label='Posterior')\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('F_true')\n",
    "plt.ylabel('Log-Likelihood')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 1  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "nburn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 2000  # number of MCMC steps to take\n",
    "\n",
    "# we'll start at random locations between 0 and 2000\n",
    "starting_guesses = 2000 * np.random.rand(nwalkers, ndim)\n",
    "\n",
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[F, e])\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    "\n",
    "sample = sampler.chain  # shape = (nwalkers, nsteps, ndim)\n",
    "sample = sampler.chain[:, nburn:, :].ravel()  # discard burn-in points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample, bins=50, histtype=\"stepfilled\", alpha=0.3, normed=True)\n",
    "\n",
    "# plot a best-fit Gaussian\n",
    "F_fit = np.linspace(F_true-2, F_true+2)\n",
    "pdf = stats.norm(np.mean(sample), np.std(sample)).pdf(F_fit)\n",
    "\n",
    "#plt.plot(F_fit, pdf, '-k')\n",
    "plt.xlabel(\"F\"); plt.ylabel(\"P(F)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "      F_true = {0}\n",
    "      F_est  = {1:.0f} +/- {2:.4f} (based on {3} measurements)\n",
    "      \"\"\".format(F_true, np.mean(sample), np.std(sample, ddof=1), N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
